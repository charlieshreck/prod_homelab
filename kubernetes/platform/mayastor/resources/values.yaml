# Mayastor configuration for 3-node Talos cluster
# Mayastor replication network: 10.50.0.0/24 (vmbr4/ens20)

# Disable observability components (loki, minio, metrics)
# Core storage functionality doesn't require these
obs:
  callhome:
    enabled: false
loki:
  enabled: false

# IO Engine configuration (data plane)
io_engine:
  hugepages:
    enabled: true
  nodeSelector:
    openebs.io/engine: mayastor
  resources:
    limits:
      cpu: "2"
      memory: "2Gi"
      hugepages-2Mi: "2Gi"
    requests:
      cpu: "1"
      memory: "2Gi"
      hugepages-2Mi: "2Gi"

agents:
  core:
    nodeSelector:
      openebs.io/engine: mayastor

csi:
  node:
    # Talos Linux kubelet path
    kubeletDir: /var/lib/kubelet
    resources:
      limits:
        cpu: "1"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"

# =============================================================================
# ETCD: hostPath persistence with proper cluster join handling
# =============================================================================
etcd:
  replicaCount: 3

  # Disable default PVC-based persistence
  persistence:
    enabled: false

  # Override command to source join env before setup
  command:
    - /bin/bash
    - -c
    - |
      # Source join configuration if it exists (provides ETCD_INITIAL_CLUSTER)
      if [ -f /bitnami/etcd/data/new_member_envs ]; then
        echo "Sourcing join configuration..."
        . /bitnami/etcd/data/new_member_envs
        export ETCD_INITIAL_CLUSTER
        export ETCD_INITIAL_ADVERTISE_PEER_URLS
      fi
      # Run the original entrypoint
      exec /opt/bitnami/scripts/etcd/entrypoint.sh /opt/bitnami/scripts/etcd/run.sh

  # Use hostPath via extraVolumes (survives pod restarts, no StorageClass needed)
  extraVolumes:
    - name: etcd-data-hostpath
      hostPath:
        path: /var/lib/mayastor-etcd
        type: DirectoryOrCreate

  extraVolumeMounts:
    - name: etcd-data-hostpath
      mountPath: /bitnami/etcd/data

  # Init containers: fix permissions and handle cluster join for non-first pods
  initContainers:
    - name: fix-permissions
      image: busybox:latest
      command:
        - sh
        - -c
        - |
          chown -R 1001:1001 /bitnami/etcd/data
          chmod 700 /bitnami/etcd/data
      volumeMounts:
        - name: etcd-data-hostpath
          mountPath: /bitnami/etcd/data
      securityContext:
        runAsUser: 0
    - name: cluster-join
      image: docker.io/openebs/etcd:3.5.6-debian-11-r10
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: ETCDCTL_API
          value: "3"
      command:
        - sh
        - -c
        - |
          set -e
          DATA_DIR=/bitnami/etcd/data
          ENV_FILE=$DATA_DIR/new_member_envs
          # Use etcd-1 and etcd-2 endpoints only (etcd-0 might not be running during join)
          CLUSTER_ENDPOINTS="http://mayastor-etcd-1.mayastor-etcd-headless.mayastor.svc.cluster.local:2379,http://mayastor-etcd-2.mayastor-etcd-headless.mayastor.svc.cluster.local:2379"
          MY_NAME=$POD_NAME
          MY_PEER_URL="http://${MY_NAME}.mayastor-etcd-headless.mayastor.svc.cluster.local:2380"

          echo "Init: POD_NAME=$POD_NAME"

          # If data already exists (member directory), skip
          if [ -d "$DATA_DIR/member" ]; then
            echo "Data directory exists, skipping join logic"
            exit 0
          fi

          # If new_member_envs already exists, skip
          if [ -f "$ENV_FILE" ]; then
            echo "Join env file exists, skipping"
            exit 0
          fi

          echo "No existing data, checking if cluster exists..."

          # Wait for cluster to be available (etcd-0 might still be starting)
          MAX_ATTEMPTS=30
          ATTEMPT=0
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if etcdctl --endpoints=$CLUSTER_ENDPOINTS endpoint health 2>/dev/null; then
              echo "Cluster is healthy"
              break
            fi
            echo "Waiting for cluster... attempt $((ATTEMPT+1))/$MAX_ATTEMPTS"
            sleep 5
            ATTEMPT=$((ATTEMPT+1))
          done

          if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
            echo "Cluster not available, will let main container handle bootstrap"
            exit 0
          fi

          # Check if we're already a member - if so, remove stale entry
          MEMBERS=$(etcdctl --endpoints=$CLUSTER_ENDPOINTS member list 2>/dev/null || echo "")
          if echo "$MEMBERS" | grep -q "$MY_NAME"; then
            echo "Found existing member entry for $MY_NAME"
            if echo "$MEMBERS" | grep "$MY_NAME" | grep -q "started"; then
              echo "Member shows as started but we have no local data - removing stale entry"
              MEMBER_ID=$(echo "$MEMBERS" | grep "$MY_NAME" | cut -d',' -f1)
              etcdctl --endpoints=$CLUSTER_ENDPOINTS member remove $MEMBER_ID 2>/dev/null || true
              sleep 2
            else
              echo "Member exists as unstarted - removing to let Bitnami re-add it"
              MEMBER_ID=$(echo "$MEMBERS" | grep "$MY_NAME" | cut -d',' -f1)
              etcdctl --endpoints=$CLUSTER_ENDPOINTS member remove $MEMBER_ID 2>/dev/null || true
              sleep 2
            fi
          fi

          # Cluster exists, Bitnami will handle member add
          echo "Cluster exists with members:"
          etcdctl --endpoints=$CLUSTER_ENDPOINTS member list 2>/dev/null || true
          echo "Bitnami entrypoint will add this node to the cluster"
      volumeMounts:
        - name: etcd-data-hostpath
          mountPath: /bitnami/etcd/data
      securityContext:
        runAsUser: 0

  # Stability tuning
  extraEnvVars:
    - name: ETCD_HEARTBEAT_INTERVAL
      value: "500"
    - name: ETCD_ELECTION_TIMEOUT
      value: "2500"
    - name: ETCD_QUOTA_BACKEND_BYTES
      value: "2147483648"

  livenessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 15
    successThreshold: 1
    failureThreshold: 5

  readinessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 15
    successThreshold: 1
    failureThreshold: 5

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - etcd
          topologyKey: kubernetes.io/hostname

  resources:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "100m"
      memory: "128Mi"

  pdb:
    create: false
